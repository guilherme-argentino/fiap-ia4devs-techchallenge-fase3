{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guilherme-argentino/fiap-ia4devs-techchallenge-fase3/blob/main/Fase3_TechChallenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UCnH-DobEf1"
      },
      "source": [
        "# Fine-tuning do Modelo BERT com AmazonTitles-1.3MM\n",
        "\n",
        "Neste notebook, realizaremos o fine-tuning do modelo BERT (`bert-base-uncased`) usando o dataset \"The AmazonTitles-1.3MM\". O objetivo é treinar o modelo para que ele consiga gerar descrições de produtos com base em seus títulos."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Instalar dependências\n",
        "\n",
        "Primeira célula: Instala as bibliotecas necessárias.\n"
      ],
      "metadata": {
        "id": "uGH0B-GzbfE5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2PvYa8bbEf3",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Instalar as bibliotecas necessárias\n",
        "!pip install datasets transformers numpy scipy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPHxsBcCbEf4"
      },
      "source": [
        "## 2. Importar as Bibliotecas\n",
        "\n",
        "Agora, importaremos as bibliotecas necessárias para nosso trabalho."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWc5mdMabEf4"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, BertForMaskedLM, Trainer, TrainingArguments\n",
        "import torch\n",
        "import gc\n",
        "import numpy as np\n",
        "import scipy.sparse as sp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjecW_dcbEf4"
      },
      "source": [
        "## 3. Montar o Google Drive\n",
        "\n",
        "Vamos montar o Google Drive para acessar os arquivos que contêm os dados de treinamento e teste."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAr7wqiBbEf5"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYE12zzPbEf5"
      },
      "source": [
        "## 4. Carregar o Dataset de Treinamento\n",
        "\n",
        "Carregaremos o dataset de treinamento (`trn.json`) utilizando a biblioteca `datasets`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Te07dFKbEf6"
      },
      "outputs": [],
      "source": [
        "# Carregar o arquivo trn.json\n",
        "jsonl_file = '/content/drive/MyDrive/FIAP/1IADT/Fase-3/LF-Amazon-1.3M/trn.json'  # Altere para o caminho correto\n",
        "\n",
        "# Ler o arquivo jsonl com pandas\n",
        "dataset = load_dataset('json', data_files=jsonl_file, split='train', streaming=True)\n",
        "\n",
        "# Exibir uma amostra dos dados\n",
        "print(next(iter(dataset)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87RhMjyTbEf6"
      },
      "source": [
        "## 5. Processar o Dataset de Treinamento\n",
        "\n",
        "Vamos processar os dados, criando prompts para o modelo com base nos títulos dos produtos e suas descrições."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKJSDYOsbEf6"
      },
      "outputs": [],
      "source": [
        "# Inicializar uma lista para armazenar os dados processados\n",
        "dados_processados = []\n",
        "\n",
        "# Iterar sobre o dataset e criar os pares de entrada e saída\n",
        "for example in dataset:\n",
        "    title = example.get('title', '')\n",
        "    content = example.get('content', '')\n",
        "\n",
        "    # Criar um prompt com base no título e na descrição\n",
        "    prompt = f\"Descreva o produto com o título '{title}'?\"\n",
        "    dados_processados.append({\"input_text\": prompt, \"output_text\": content})\n",
        "\n",
        "    # Opcionalmente, pare após processar um certo número de exemplos\n",
        "    # if len(dados_processados) >= 10000:\n",
        "        # break  # Ajuste este valor conforme necessário para processar mais exemplos\n",
        "\n",
        "# Mostrar uma amostra dos dados processados\n",
        "dados_processados[:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gv8WOXjabEf7"
      },
      "source": [
        "## ## 6. Tokenizar os Dados de Treinamento em Lotes\n",
        "\n",
        "Utilizaremos o `AutoTokenizer` da biblioteca `transformers` para tokenizar os dados de entrada e saída em lotes menores.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bj_7UJhDbEf7"
      },
      "outputs": [],
      "source": [
        "# Carregar o tokenizer do BERT (bert-base-uncased) com uso rápido\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=True, clean_up_tokenization_spaces=True)\n",
        "\n",
        "batch_size = 16  # Tamanho do lote reduzido\n",
        "\n",
        "def tokenize_in_batches(dataset, batch_size):\n",
        "    tokenized_data = []\n",
        "    for i in range(0, len(dataset), batch_size):\n",
        "        batch = dataset[i:i + batch_size]\n",
        "        # Tokenização com limpeza de espaços controlada\n",
        "        inputs = tokenizer(\n",
        "            [example['input_text'] for example in batch],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=256\n",
        "        )\n",
        "        tokenized_data.append(inputs)\n",
        "    return tokenized_data\n",
        "\n",
        "# Usar apenas um subconjunto para teste (opcional)\n",
        "dados_processados = dados_processados[:100000]  # Use as primeiras 1000 amostras\n",
        "\n",
        "# Tokenizar os dados processados\n",
        "dados_tokenizados = tokenize_in_batches(dados_processados, batch_size)\n",
        "\n",
        "# Limpar variáveis não necessárias\n",
        "del dados_processados\n",
        "gc.collect()\n",
        "\n",
        "# Exemplo de um registro tokenizado\n",
        "print(dados_tokenizados[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxfiPJdjbEf8"
      },
      "source": [
        "## 7. Criar o Dataset de Treinamento\n",
        "\n",
        "Agora, criaremos o dataset de treinamento e dividiremos em conjuntos de treino e validação."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_zJToBtbEf8"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# Converter os dados tokenizados em um Dataset da Hugging Face\n",
        "hf_dataset = Dataset.from_list(dados_tokenizados)\n",
        "train_test_split = hf_dataset.train_test_split(test_size=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKAiDqC6bEf8"
      },
      "source": [
        "## 8. Configurar e Treinar o Modelo\n",
        "\n",
        "Iremos configurar o modelo BERT para o fine-tuning e definir os parâmetros de treinamento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJp5A0HtbEf8"
      },
      "outputs": [],
      "source": [
        "# Carregar o modelo BERT para Masked Language Modeling\n",
        "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Definir os argumentos de treinamento\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=500,\n",
        "    save_steps=1000,\n",
        "    num_train_epochs=3,\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "# Inicializar o Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_test_split['train'],\n",
        "    eval_dataset=train_test_split['test'],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQ_ztMFybEf8"
      },
      "source": [
        "## 9. Treinar o Modelo\n",
        "\n",
        "Agora vamos iniciar o treinamento do modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cy_aOAVpbEf9"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTV1IJrybEf9"
      },
      "source": [
        "## 10. Carregar o Conjunto de Teste\n",
        "\n",
        "Em seguida, carregaremos o conjunto de teste (`tst.json`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9k3a6uobEf9"
      },
      "outputs": [],
      "source": [
        "tst_json_file = '/content/drive/My Drive/caminho_para_o_seu_arquivo/tst.json'  # Altere para o caminho correto\n",
        "test_dataset = load_dataset('json', data_files=tst_json_file, split='train')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMh68qkxbEf9"
      },
      "source": [
        "## 11. Processar o Conjunto de Teste\n",
        "\n",
        "Processaremos o conjunto de teste da mesma forma que fizemos com o conjunto de treinamento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxmqLSy_bEf9"
      },
      "outputs": [],
      "source": [
        "test_dados_processados = []\n",
        "for example in test_dataset:\n",
        "    title = example.get('title', '')\n",
        "    prompt = f\"Descreva o produto com o título '{title}'?\"\n",
        "    test_dados_processados.append({\"input_text\": prompt})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEq2lfSlbEf-"
      },
      "source": [
        "## 12. Tokenizar o Conjunto de Teste\n",
        "\n",
        "Tokenizaremos o conjunto de teste para que possamos usá-lo na avaliação do modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "So2ILf2ubEf-"
      },
      "outputs": [],
      "source": [
        "test_dados_tokenizados = [tokenizer(example['input_text'], padding=\"max_length\", truncation=True, max_length=512) for example in test_dados_processados]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gAVvPupbEf-"
      },
      "source": [
        "## 13. Avaliar o Modelo\n",
        "\n",
        "Iremos avaliar o modelo utilizando o conjunto de teste que preparamos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZgHlsOLbEf-"
      },
      "outputs": [],
      "source": [
        "def gerar_respostas(model, tokenized_inputs):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**tokenized_inputs)\n",
        "    return outputs\n",
        "\n",
        "respostas = []\n",
        "for tokenized_input in test_dados_tokenizados:\n",
        "    resposta = gerar_respostas(model, tokenized_input)\n",
        "    respostas.append(resposta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5RT0IPrbEf-"
      },
      "source": [
        "## 14. Exibir Algumas Respostas Geradas\n",
        "\n",
        "Por fim, exibiremos algumas respostas geradas pelo modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crL029Z3bEf-"
      },
      "outputs": [],
      "source": [
        "print(respostas[:5])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}