{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guilherme-argentino/fiap-ia4devs-techchallenge-fase3/blob/main/Fase3_TechChallenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UCnH-DobEf1"
      },
      "source": [
        "# Fine-tuning do Modelo BERT com AmazonTitles-1.3MM\n",
        "\n",
        "Neste notebook, realizaremos o fine-tuning do modelo BERT (`bert-base-uncased`) usando o dataset \"The AmazonTitles-1.3MM\". O objetivo é treinar o modelo para que ele consiga gerar descrições de produtos com base em seus títulos."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Instalar dependências\n",
        "\n",
        "Primeira célula: Instala as bibliotecas necessárias.\n"
      ],
      "metadata": {
        "id": "uGH0B-GzbfE5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "P2PvYa8bbEf3",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f15eee24-526b-4e14-982a-1d5c38253304"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in ./miniconda3/lib/python3.12/site-packages (3.0.1)\r\n",
            "Requirement already satisfied: transformers in ./miniconda3/lib/python3.12/site-packages (4.45.1)\r\n",
            "Requirement already satisfied: numpy in ./miniconda3/lib/python3.12/site-packages (1.26.4)\r\n",
            "Requirement already satisfied: scipy in ./miniconda3/lib/python3.12/site-packages (1.14.1)\n",
            "Requirement already satisfied: filelock in ./miniconda3/lib/python3.12/site-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in ./miniconda3/lib/python3.12/site-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./miniconda3/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in ./miniconda3/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in ./miniconda3/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in ./miniconda3/lib/python3.12/site-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in ./miniconda3/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in ./miniconda3/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in ./miniconda3/lib/python3.12/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in ./miniconda3/lib/python3.12/site-packages (from datasets) (3.10.7)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in ./miniconda3/lib/python3.12/site-packages (from datasets) (0.25.1)\n",
            "Requirement already satisfied: packaging in ./miniconda3/lib/python3.12/site-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in ./miniconda3/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in ./miniconda3/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in ./miniconda3/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in ./miniconda3/lib/python3.12/site-packages (from transformers) (0.20.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in ./miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in ./miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in ./miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in ./miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in ./miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./miniconda3/lib/python3.12/site-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in ./miniconda3/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in ./miniconda3/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in ./miniconda3/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in ./miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# Instalar as bibliotecas necessárias\n",
        "!pip install datasets transformers numpy scipy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPHxsBcCbEf4"
      },
      "source": [
        "## 2. Importar as Bibliotecas e criar as funções utilitárias\n",
        "\n",
        "Agora, importaremos as bibliotecas necessárias para nosso trabalho. Além de criar funções para executar em nosso processo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VWc5mdMabEf4"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, BertForMaskedLM, Trainer, TrainingArguments\n",
        "import torch\n",
        "import gc, os\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import pandas, gzip\n",
        "\n",
        "def to_scipy_matrix(lol_inds, num_cols):\n",
        "  cols = np.concatenate(lol_inds)\n",
        "  rows = np.concatenate([[i]*len(x) for i, x in enumerate(lol_inds)])\n",
        "  data = np.concatenate([[1.0]*len(x) for i, x in enumerate(lol_inds)])\n",
        "  return sp.coo_matrix((data, (rows, cols)), (len(lol_inds), num_cols)).tocsr()\n",
        "\n",
        "def process_lf_amazon_datasets(dataset):\n",
        "  print('Reading raw dataset files...')\n",
        "  trn_df = pandas.read_json(gzip.open(f'Datasets/{dataset}/trn.json.gz'), lines=True)\n",
        "  tst_df = pandas.read_json(gzip.open(f'Datasets/{dataset}/tst.json.gz'), lines=True)\n",
        "  lbl_df = pandas.read_json(gzip.open(f'Datasets/{dataset}/lbl.json.gz'), lines=True)\n",
        "\n",
        "  print('Processing Y (label) files...')\n",
        "  trn_X_Y = to_scipy_matrix(trn_df.target_ind.values, lbl_df.shape[0])\n",
        "  tst_X_Y = to_scipy_matrix(tst_df.target_ind.values, lbl_df.shape[0])\n",
        "\n",
        "  sp.save_npz(f'Datasets/{dataset}/Y.trn.npz', trn_X_Y)\n",
        "  sp.save_npz(f'Datasets/{dataset}/Y.tst.npz', tst_X_Y)\n",
        "\n",
        "  print('Processing X (input) files...')\n",
        "  print(*trn_df.title.apply(lambda x: x.strip()).values, sep='\\n', file=open(f'Datasets/{dataset}/raw/trn_X.txt', 'w'))\n",
        "  print(*tst_df.title.apply(lambda x: x.strip()).values, sep='\\n', file=open(f'Datasets/{dataset}/raw/tst_X.txt', 'w'))\n",
        "  print(*lbl_df.title.apply(lambda x: x.strip()).values, sep='\\n', file=open(f'Datasets/{dataset}/raw/Y.txt', 'w'))\n",
        "\n",
        "  #print('Tokenizing X (input) files...')\n",
        "  #max_len = 32 if 'titles' in dataset else 128\n",
        "  #os.system(f\"python utils/tokenization_utils.py --data-path Datasets/{dataset}/raw/trn_X.txt --tf-max-len {max_len}\")\n",
        "  #os.system(f\"python utils/tokenization_utils.py --data-path Datasets/{dataset}/raw/tst_X.txt --tf-max-len {max_len}\")\n",
        "  #os.system(f\"python utils/tokenization_utils.py --data-path Datasets/{dataset}/raw/Y.txt --tf-max-len {max_len}\")\n",
        "\n",
        "  return (trn_df, tst_df, lbl_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjecW_dcbEf4"
      },
      "source": [
        "## 3. Baixar dados o Google Drive\n",
        "\n",
        "Vamos baixar os dados do Google Drive para acessar os arquivos que contêm os dados de treinamento e teste."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZAr7wqiBbEf5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e28c2a86-93a4-4b7b-ab47-b388f4a752ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\r\n",
            "From (original): https://drive.google.com/uc?id=12zH4mL2RX8iSvH0VCNnd3QxO4DzuHWnK\r\n",
            "From (redirected): https://drive.google.com/uc?id=12zH4mL2RX8iSvH0VCNnd3QxO4DzuHWnK&confirm=t&uuid=e7e82f98-649f-4167-878e-9adca4b955bd\r\n",
            "To: /home/guilherme/Datasets/LF-Amazon-1.3M.raw.zip\n",
            "100%|████████████████████████████████████████| 890M/890M [01:01<00:00, 14.5MB/s]\n",
            "Archive:  LF-Amazon-1.3M.raw.zip\n",
            "   creating: LF-Amazon-1.3M/\n",
            "  inflating: LF-Amazon-1.3M/lbl.json.gz  \n",
            "  inflating: LF-Amazon-1.3M/trn.json.gz  \n",
            "  inflating: LF-Amazon-1.3M/filter_labels_test.txt  \n",
            "  inflating: LF-Amazon-1.3M/tst.json.gz  \n",
            "  inflating: LF-Amazon-1.3M/filter_labels_train.txt  \n"
          ]
        }
      ],
      "source": [
        "!mkdir -p Datasets\n",
        "!cd Datasets; gdown 12zH4mL2RX8iSvH0VCNnd3QxO4DzuHWnK;\n",
        "!cd Datasets; unzip LF-Amazon-1.3M.raw.zip; mkdir -p LF-AmazonTitles-1.3M/raw; mv LF-Amazon-1.3M/* LF-AmazonTitles-1.3M"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYE12zzPbEf5"
      },
      "source": [
        "## 4. Carregar o Dataset de Treinamento\n",
        "\n",
        "Carregaremos o dataset de treinamento (`trn.json`) utilizando a biblioteca `datasets`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Te07dFKbEf6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1edae014-232c-4665-b645-ea2ba9b59fce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading raw dataset files...\n"
          ]
        }
      ],
      "source": [
        "# Ler o arquivo jsonl com pandas\n",
        "(dataset, tst_df , lbl_df) = process_lf_amazon_datasets('LF-AmazonTitles-1.3M')\n",
        "\n",
        "# Exibir uma amostra dos dados\n",
        "print(dataset.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87RhMjyTbEf6"
      },
      "source": [
        "## 5. Processar o Dataset de Treinamento\n",
        "\n",
        "Vamos processar os dados, criando prompts para o modelo com base nos títulos dos produtos e suas descrições."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKJSDYOsbEf6"
      },
      "outputs": [],
      "source": [
        "# Inicializar uma lista para armazenar os dados processados\n",
        "dados_processados = []\n",
        "\n",
        "# Iterar sobre o dataset e criar os pares de entrada e saída\n",
        "for example in dataset:\n",
        "    title = example.get('title', '')\n",
        "    content = example.get('content', '')\n",
        "\n",
        "    # Criar um prompt com base no título e na descrição\n",
        "    prompt = f\"Descreva o produto com o título '{title}'?\"\n",
        "    dados_processados.append({\"input_text\": prompt, \"output_text\": content})\n",
        "\n",
        "    # Opcionalmente, pare após processar um certo número de exemplos\n",
        "    # if len(dados_processados) >= 10000:\n",
        "        # break  # Ajuste este valor conforme necessário para processar mais exemplos\n",
        "\n",
        "# Mostrar uma amostra dos dados processados\n",
        "dados_processados[:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gv8WOXjabEf7"
      },
      "source": [
        "## 6. Tokenizar os Dados de Treinamento em Lotes\n",
        "\n",
        "Utilizaremos o `AutoTokenizer` da biblioteca `transformers` para tokenizar os dados de entrada e saída em lotes menores.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bj_7UJhDbEf7"
      },
      "outputs": [],
      "source": [
        "# Carregar o tokenizer do BERT (bert-base-uncased) com uso rápido\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=True, clean_up_tokenization_spaces=True)\n",
        "\n",
        "batch_size = 16  # Tamanho do lote reduzido\n",
        "\n",
        "def tokenize_in_batches(dataset, batch_size):\n",
        "    tokenized_data = []\n",
        "    for i in range(0, len(dataset), batch_size):\n",
        "        batch = dataset[i:i + batch_size]\n",
        "        # Tokenização com limpeza de espaços controlada\n",
        "        inputs = tokenizer(\n",
        "            [example['input_text'] for example in batch],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=256\n",
        "        )\n",
        "        tokenized_data.append(inputs)\n",
        "    return tokenized_data\n",
        "\n",
        "# Usar apenas um subconjunto para teste (opcional)\n",
        "dados_processados = dados_processados[:100000]  # Use as primeiras 1000 amostras\n",
        "\n",
        "# Tokenizar os dados processados\n",
        "dados_tokenizados = tokenize_in_batches(dados_processados, batch_size)\n",
        "\n",
        "# Limpar variáveis não necessárias\n",
        "del dados_processados\n",
        "gc.collect()\n",
        "\n",
        "# Exemplo de um registro tokenizado\n",
        "print(dados_tokenizados[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxfiPJdjbEf8"
      },
      "source": [
        "## 7. Criar o Dataset de Treinamento\n",
        "\n",
        "Agora, criaremos o dataset de treinamento e dividiremos em conjuntos de treino e validação."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_zJToBtbEf8"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# Converter os dados tokenizados em um Dataset da Hugging Face\n",
        "hf_dataset = Dataset.from_list(dados_tokenizados)\n",
        "train_test_split = hf_dataset.train_test_split(test_size=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKAiDqC6bEf8"
      },
      "source": [
        "## 8. Configurar e Treinar o Modelo\n",
        "\n",
        "Iremos configurar o modelo BERT para o fine-tuning e definir os parâmetros de treinamento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJp5A0HtbEf8"
      },
      "outputs": [],
      "source": [
        "# Carregar o modelo BERT para Masked Language Modeling\n",
        "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Definir os argumentos de treinamento\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=500,\n",
        "    save_steps=1000,\n",
        "    num_train_epochs=3,\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "# Inicializar o Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_test_split['train'],\n",
        "    eval_dataset=train_test_split['test'],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQ_ztMFybEf8"
      },
      "source": [
        "## 9. Treinar o Modelo\n",
        "\n",
        "Agora vamos iniciar o treinamento do modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cy_aOAVpbEf9"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTV1IJrybEf9"
      },
      "source": [
        "## 10. Carregar o Conjunto de Teste\n",
        "\n",
        "Em seguida, carregaremos o conjunto de teste (`tst.json`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9k3a6uobEf9"
      },
      "outputs": [],
      "source": [
        "tst_json_file = '/content/drive/My Drive/caminho_para_o_seu_arquivo/tst.json'  # Altere para o caminho correto\n",
        "test_dataset = load_dataset('json', data_files=tst_json_file, split='train')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMh68qkxbEf9"
      },
      "source": [
        "## 11. Processar o Conjunto de Teste\n",
        "\n",
        "Processaremos o conjunto de teste da mesma forma que fizemos com o conjunto de treinamento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxmqLSy_bEf9"
      },
      "outputs": [],
      "source": [
        "test_dados_processados = []\n",
        "for example in test_dataset:\n",
        "    title = example.get('title', '')\n",
        "    prompt = f\"Descreva o produto com o título '{title}'?\"\n",
        "    test_dados_processados.append({\"input_text\": prompt})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEq2lfSlbEf-"
      },
      "source": [
        "## 12. Tokenizar o Conjunto de Teste\n",
        "\n",
        "Tokenizaremos o conjunto de teste para que possamos usá-lo na avaliação do modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "So2ILf2ubEf-"
      },
      "outputs": [],
      "source": [
        "test_dados_tokenizados = [tokenizer(example['input_text'], padding=\"max_length\", truncation=True, max_length=512) for example in test_dados_processados]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gAVvPupbEf-"
      },
      "source": [
        "## 13. Avaliar o Modelo\n",
        "\n",
        "Iremos avaliar o modelo utilizando o conjunto de teste que preparamos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZgHlsOLbEf-"
      },
      "outputs": [],
      "source": [
        "def gerar_respostas(model, tokenized_inputs):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**tokenized_inputs)\n",
        "    return outputs\n",
        "\n",
        "respostas = []\n",
        "for tokenized_input in test_dados_tokenizados:\n",
        "    resposta = gerar_respostas(model, tokenized_input)\n",
        "    respostas.append(resposta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5RT0IPrbEf-"
      },
      "source": [
        "## 14. Exibir Algumas Respostas Geradas\n",
        "\n",
        "Por fim, exibiremos algumas respostas geradas pelo modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crL029Z3bEf-"
      },
      "outputs": [],
      "source": [
        "print(respostas[:5])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}