{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNi1dzZNL4rj2N/lk9fRG7B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guilherme-argentino/fiap-ia4devs-techchallenge-fase3/blob/main/Fase3_TechChallenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tech Challenge Fase 3\n",
        "\n",
        "Vamos usar o modelo **BERT** no lugar de GPT-2, especificamente o modelo **`bert-base-uncased`** da Hugging Face, neste processo de fine-tuning muda um pouco, já que o BERT é um modelo \"Masked Language Model\" (MLM), e geralmente é utilizado para tarefas como classificação, predição de token, ou tarefas como QA (perguntas e respostas).\n",
        "\n",
        "Vamos montar o notebook para usar o **BERT**. O objetivo é ajustar o modelo para responder a perguntas com base nas descrições dos produtos, o que pode ser tratado como uma tarefa de classificação de sequência (entrada e saída).\n"
      ],
      "metadata": {
        "id": "N1_kiGtGNX3H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Instalar dependências\n",
        "\n",
        "Primeira célula: Instala as bibliotecas necessárias."
      ],
      "metadata": {
        "id": "YrYFeJyKNjYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets pandas"
      ],
      "metadata": {
        "id": "LhO8Z4XyNmO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Carregar e ler o arquivo JSONL com pandas\n",
        "\n",
        "Usando o pandas para carregar o arquivo `trn.json` no formato JSONL. Certifique-se de fazer upload do arquivo no Colab ou fornecer o caminho correto.\n"
      ],
      "metadata": {
        "id": "wITJ7ybmNopV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Carregar o arquivo trn.json\n",
        "jsonl_file = '/content/trn.json'  # Ajuste o caminho conforme necessário\n",
        "\n",
        "# Ler o arquivo jsonl com pandas\n",
        "df = pd.read_json(jsonl_file, lines=True)\n",
        "\n",
        "# Exibir uma amostra dos dados\n",
        "df.head()"
      ],
      "metadata": {
        "id": "5jDQElBfNvzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Selecionar as colunas relevantes\n",
        "\n",
        "Aqui, selecionamos as colunas **\"title\"** (título do produto) e **\"content\"** (descrição do produto) para criar pares de entrada e saída.\n"
      ],
      "metadata": {
        "id": "4_EGrCD_N0Cd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecionar as colunas title e content\n",
        "df = df[['title', 'content']]\n",
        "\n",
        "# Exibir uma amostra dos dados filtrados\n",
        "df.head()"
      ],
      "metadata": {
        "id": "PPf6ZodRN3eu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Preparar os dados para o fine-tuning\n",
        "\n",
        "Vamos criar pares de input-output para o modelo de BERT. Aqui, o input será a pergunta (\"Descreva o produto com o título X?\") e o output será a descrição do produto.\n"
      ],
      "metadata": {
        "id": "URF_xYzRN_7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para criar prompts no formato correto\n",
        "def create_prompt(row):\n",
        "    prompt = f\"Descreva o produto com o título '{row['title']}'?\"\n",
        "    return {\"input_text\": prompt, \"output_text\": row['content']}\n",
        "\n",
        "# Aplicar a função a cada linha do dataframe\n",
        "dataset = df.apply(create_prompt, axis=1)\n",
        "\n",
        "# Converter para um DataFrame do Hugging Face\n",
        "from datasets import Dataset\n",
        "hf_dataset = Dataset.from_pandas(dataset)\n",
        "\n",
        "# Exibir uma amostra do dataset final\n",
        "hf_dataset[0]"
      ],
      "metadata": {
        "id": "dN4EX5n6OC2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Tokenização e preparação para o BERT\n",
        "\n",
        "Como estamos usando o **BERT**, é importante garantir que os dados de entrada sejam tokenizados corretamente. Usaremos o tokenizador de **`bert-base-uncased`** e tokenizaremos tanto a pergunta quanto a resposta."
      ],
      "metadata": {
        "id": "MnLq5Q-7OGjm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Carregar o tokenizador do BERT\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Função para tokenizar a entrada e a saída\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['input_text'], padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "# Aplicar a tokenização ao dataset\n",
        "tokenized_dataset = hf_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Exibir uma amostra dos dados tokenizados\n",
        "tokenized_dataset[0]"
      ],
      "metadata": {
        "id": "NaNS0WHVOLVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Preparar o modelo BERT para o fine-tuning\n",
        "\n",
        "Vamos carregar o modelo **`bert-base-uncased`** e adaptá-lo para uma tarefa de geração de texto. Para essa tarefa, usaremos a classe `AutoModelForSequenceClassification`, pois estamos tentando gerar uma saída baseada em uma entrada de sequência."
      ],
      "metadata": {
        "id": "ikL-OC7gORrt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "# Carregar o modelo BERT pré-treinado para classificação de sequência\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=1)"
      ],
      "metadata": {
        "id": "QObb75a2OU_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Configurar o treinamento usando o Trainer da Hugging Face\n",
        "\n",
        "Agora configuramos o treinamento. Ajustamos os argumentos de treinamento, como o número de épocas e o tamanho do batch."
      ],
      "metadata": {
        "id": "_rCp5Jd_OXms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "# Definir os parâmetros de treinamento\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    logging_dir=\"./logs\",\n",
        ")\n",
        "\n",
        "# Configurar o Trainer para o BERT\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        ")\n",
        "\n",
        "# Executar o treinamento\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "P_ESEX2gOa2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Avaliação do modelo após o fine-tuning\n",
        "\n",
        "Após o treinamento, podemos testar o modelo para gerar uma descrição com base em um novo título fornecido pelo usuário."
      ],
      "metadata": {
        "id": "eJbCtrAbOgZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pergunta do usuário\n",
        "user_input = \"Descreva o produto com o título 'Headphones Bluetooth XYZ'?\"\n",
        "\n",
        "# Tokenizar a pergunta\n",
        "inputs = tokenizer(user_input, return_tensors=\"pt\")\n",
        "\n",
        "# Gerar a resposta com o modelo fine-tunado\n",
        "outputs = model(**inputs)\n",
        "\n",
        "# Exibir o resultado\n",
        "print(\"Saída:\", outputs)"
      ],
      "metadata": {
        "id": "FWLEzX8xOjJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Conclusão e Salvamento do Modelo\n",
        "\n",
        "Por fim, você pode salvar o modelo treinado para usá-lo posteriormente."
      ],
      "metadata": {
        "id": "IaMHBW0iOoge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Salvar o modelo fine-tunado\n",
        "model.save_pretrained(\"./fine_tuned_bert_model\")\n",
        "tokenizer.save_pretrained(\"./fine_tuned_bert_model\")\n",
        "\n",
        "print(\"Modelo treinado salvo!\")"
      ],
      "metadata": {
        "id": "O0btkO37OrIN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}