{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guilherme-argentino/fiap-ia4devs-techchallenge-fase3/blob/main/Fase3_TechChallenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UCnH-DobEf1"
      },
      "source": [
        "# Fine-tuning do Modelo BERT com AmazonTitles-1.3MM\n",
        "\n",
        "Neste notebook, realizaremos o fine-tuning do modelo BERT (`bert-base-uncased`) usando o dataset \"The AmazonTitles-1.3MM\". O objetivo é treinar o modelo para que ele consiga gerar descrições de produtos com base em seus títulos."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Instalar dependências\n"
      ],
      "metadata": {
        "id": "uGH0B-GzbfE5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "P2PvYa8bbEf3",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "717eeca2-280f-41e7-cb64-5e7e82dc8377"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (3.0.1)\r\n",
            "Requirement already satisfied: transformers in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (4.45.1)\r\n",
            "Requirement already satisfied: torch in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (2.4.1)\r\n",
            "Requirement already satisfied: pandas in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (2.2.3)\n",
            "Collecting gdown\n",
            "  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from datasets) (2.1.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from datasets) (3.10.8)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from datasets) (0.25.1)\n",
            "Requirement already satisfied: packaging in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from transformers) (0.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from torch) (72.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from transformers[torch]) (0.34.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: psutil in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from accelerate>=0.26.0->transformers[torch]) (6.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from aiohttp->datasets) (1.13.1)\n",
            "Requirement already satisfied: six>=1.5 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
            "Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: gdown\n",
            "Successfully installed gdown-5.2.0\n"
          ]
        }
      ],
      "source": [
        "# Instalar as bibliotecas necessárias\n",
        "!pip install datasets transformers torch pandas 'transformers[torch]' gdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPHxsBcCbEf4"
      },
      "source": [
        "## 2. Importar as Bibliotecas e preparar o Ambiente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "VWc5mdMabEf4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06201c0c-8e73-4108-bcf1-36b0736a339b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import torch\n",
        "import gzip\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "\n",
        "# Verificar se temos acesso a uma GPU no Colab\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Carregar o Tokenizer e o Modelo BERT"
      ],
      "metadata": {
        "id": "Me9kbqRLaCZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar o tokenizer BERT\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Carregar o modelo BERT para classificação\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-V8tXruaO1G",
        "outputId": "c82a9b18-7312-4414-faa0-3a2c1a39d2e2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Classe Dataset para Gerenciamento de Dados"
      ],
      "metadata": {
        "id": "5Ht_ZUpAaTg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AmazonTitlesDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n"
      ],
      "metadata": {
        "id": "8nKP8fj4aVmu"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5. Leitura em Chunks e Tokenização (para JSONL compactado em GZIP)\n",
        "Vamos ajustar a função de leitura para processar arquivos JSONL. Cada linha é um objeto JSON separado, então a função simplesmente lê uma linha por vez e cria blocos (chunks)."
      ],
      "metadata": {
        "id": "ZcEMaltPaZDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ler_arquivo_em_blocos_jsonl_gz(caminho_arquivo, tamanho_bloco=10000):\n",
        "    with gzip.open(caminho_arquivo, 'rt') as f:  # 'rt' para ler como texto\n",
        "        bloco = []\n",
        "        for i, linha in enumerate(f):\n",
        "            bloco.append(json.loads(linha.strip()))  # Lê uma linha como JSON\n",
        "            if (i + 1) % tamanho_bloco == 0:\n",
        "                yield bloco\n",
        "                bloco = []\n",
        "        if bloco:\n",
        "            yield bloco\n",
        "\n",
        "def processar_e_tokenizar_chunk(chunk, max_length=128):\n",
        "    titles = [item['title'] for item in chunk]\n",
        "    descriptions = [item['content'] for item in chunk]\n",
        "\n",
        "    # Concatenar título e descrição\n",
        "    inputs = [f\"{title} [SEP] {description}\" for title, description in zip(titles, descriptions)]\n",
        "\n",
        "    # Tokenização\n",
        "    encodings = tokenizer(inputs, truncation=True, padding=True, max_length=max_length)\n",
        "\n",
        "    # Exemplo de rótulos fictícios; substitua conforme necessário\n",
        "    labels = [1] * len(chunk)\n",
        "\n",
        "    return encodings, labels\n",
        "\n"
      ],
      "metadata": {
        "id": "2dAULVlHacy_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Configuração do Treinamento\n"
      ],
      "metadata": {
        "id": "4gstltPAamtS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # Diretório de saída para os resultados\n",
        "    num_train_epochs=3,              # Número de épocas\n",
        "    per_device_train_batch_size=16,  # Tamanho do batch\n",
        "    save_steps=1000,                 # Salvar checkpoints a cada 1000 passos\n",
        "    save_total_limit=2,              # Limite de dois checkpoints salvos\n",
        "    logging_dir='./logs',            # Diretório de logs\n",
        ")\n"
      ],
      "metadata": {
        "id": "QWpJ9PD-aqh2"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Função de Treinamento por Chunk\n"
      ],
      "metadata": {
        "id": "78Pr9BUcayOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def treinar_com_chunk(encodings, labels):\n",
        "    dataset = AmazonTitlesDataset(encodings, labels)\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset,\n",
        "    )\n",
        "    # Treina o modelo usando o chunk atual\n",
        "    trainer.train()\n"
      ],
      "metadata": {
        "id": "a7aFybBgazRG"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjecW_dcbEf4"
      },
      "source": [
        "## 8. Baixar dados o Google Drive\n",
        "\n",
        "Vamos baixar os dados do Google Drive para acessar os arquivos que contêm os dados de treinamento e teste."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ZAr7wqiBbEf5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3a54f06-c7cc-413e-f023-5739389dee23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\r\n",
            "From (original): https://drive.google.com/uc?id=12zH4mL2RX8iSvH0VCNnd3QxO4DzuHWnK\r\n",
            "From (redirected): https://drive.google.com/uc?id=12zH4mL2RX8iSvH0VCNnd3QxO4DzuHWnK&confirm=t&uuid=f4ed60c7-2a6b-4318-bca7-855f151c061f\r\n",
            "To: /Users/argentino/Datasets/LF-Amazon-1.3M.raw.zip\n",
            "100%|████████████████████████████████████████| 890M/890M [00:31<00:00, 27.8MB/s]\n",
            "Archive:  LF-Amazon-1.3M.raw.zip\n",
            "   creating: LF-Amazon-1.3M/\n",
            "  inflating: LF-Amazon-1.3M/lbl.json.gz  \n",
            "  inflating: LF-Amazon-1.3M/trn.json.gz  \n",
            "  inflating: LF-Amazon-1.3M/filter_labels_test.txt  \n",
            "  inflating: LF-Amazon-1.3M/tst.json.gz  \n",
            "  inflating: LF-Amazon-1.3M/filter_labels_train.txt  \n"
          ]
        }
      ],
      "source": [
        "!mkdir -p Datasets\n",
        "!cd Datasets; gdown 12zH4mL2RX8iSvH0VCNnd3QxO4DzuHWnK;\n",
        "!cd Datasets; unzip LF-Amazon-1.3M.raw.zip; mkdir -p LF-AmazonTitles-1.3M/raw; mv LF-Amazon-1.3M/* LF-AmazonTitles-1.3M"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Processar e Treinar em Chunks\n",
        "A função principal que faz a leitura do arquivo JSONL em chunks e realiza o fine-tuning do modelo BERT em cada chunk."
      ],
      "metadata": {
        "id": "p9PvllxxbBUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Processar o arquivo compactado trn.json.gz e realizar o fine-tuning em chunks\n",
        "caminho_arquivo = 'Datasets/LF-AmazonTitles-1.3M/trn.json.gz'\n",
        "\n",
        "for i, chunk in enumerate(ler_arquivo_em_blocos_jsonl_gz(caminho_arquivo, tamanho_bloco=10000)):\n",
        "    print(f\"Processando chunk {i+1}\")\n",
        "\n",
        "    encodings, labels = processar_e_tokenizar_chunk(chunk)\n",
        "\n",
        "    # Executar treinamento no chunk atual\n",
        "    treinar_com_chunk(encodings, labels)\n",
        "\n",
        "    print(f\"Treinamento com chunk {i+1} completo.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "Z0Lxn5E4bFpW",
        "outputId": "490e2cfc-5a29-4b7c-8baa-2265ee34c4ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processando chunk 1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1365' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1365/1875 11:02 < 04:07, 2.06 it/s, Epoch 2.18/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.003100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Salvar o Modelo Fine-Tuned\n",
        "Depois de processar todos os chunks e realizar o fine-tuning do modelo, salvamos o modelo treinado."
      ],
      "metadata": {
        "id": "RKGUvtpIbLQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Salvar o modelo fine-tuned e o tokenizer\n",
        "model.save_pretrained('./1IADT')\n",
        "tokenizer.save_pretrained('./1IADT')\n",
        "\n",
        "print(\"Modelo fine-tuned salvo com sucesso.\")\n"
      ],
      "metadata": {
        "id": "X8l8d-kubPEG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}